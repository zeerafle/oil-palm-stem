{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Oil Palm Stem Classification using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:52:53.457303200Z",
     "start_time": "2023-07-19T06:52:53.405299300Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import splitfolders\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2, InceptionResNetV2, InceptionV3\n",
    "\n",
    "import cv2\n",
    "from skimage.color import rgb2lab\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "import os\n",
    "\n",
    "DATA_PATH = os.path.join('data', 'selected_dataset_3000')\n",
    "OUTPUT_PATH = os.path.join('data', 'split_selected_dataset_3000')\n",
    "TRAIN_PATH = os.path.join(OUTPUT_PATH, 'train')\n",
    "TEST_PATH = os.path.join(OUTPUT_PATH, 'test')\n",
    "VAL_PATH = os.path.join(OUTPUT_PATH, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Split to train and test\n",
    "\n",
    "Train will be used for train and validation, while test is used to measure all 6 cnn architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:53:57.822859900Z",
     "start_time": "2023-07-19T06:52:58.478032300Z"
    }
   },
   "outputs": [],
   "source": [
    "splitfolders.ratio(DATA_PATH, output=OUTPUT_PATH, seed=42, ratio=(.8,.1,.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocess and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T05:57:03.686028Z",
     "start_time": "2023-07-19T05:57:03.666027900Z"
    }
   },
   "outputs": [],
   "source": [
    "def enhance(image: Union[str, np.ndarray], display: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Enhance image by using adaptive histogram equalization\n",
    "    :param display: bool, if true then return converted image in RGB, if not, return as BGR because it needs to continue to another preprocessing\n",
    "    :param image: [str, np.ndarray], path to image file or image array in numpy array\n",
    "    :return: np.ndarray, numpy array of enhance image\n",
    "    \"\"\"\n",
    "    if type(image) is str:\n",
    "        image = cv2.imread(image)\n",
    "    image = np.uint8(cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX))\n",
    "    # convert from BGR to YCrCb\n",
    "    ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCR_CB) # create clahe object\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    # equalize the histogram of the Y channel\n",
    "    ycrcb[:, :, 0] = clahe.apply(ycrcb[:, :, 0])\n",
    "    # convert the YCR_CB image back to RGB format\n",
    "    if display:\n",
    "        return cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2RGB)\n",
    "    else:\n",
    "        return cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T05:57:03.697025700Z",
     "start_time": "2023-07-19T05:57:03.683028700Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_noise(image: Union[str, np.ndarray], filter: str = 'bilateral', display: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Remove noise from image using bilateral filter\n",
    "    :param image: [str, np.ndarray], path to image file or image array in numpy array\n",
    "    :return: np.ndarray, numpy array of image with removed noise\n",
    "    \"\"\"\n",
    "    if type(image) is str:\n",
    "        image = cv2.imread(image)\n",
    "    # apply bilateral filter with d = 15, sigmaColor = sigmaSpace = 75.\n",
    "    if filter == 'bilateral':\n",
    "        filtered_image = cv2.bilateralFilter(image, 10, 65, 65)\n",
    "    elif filter == 'median':\n",
    "        filtered_image = cv2.medianBlur(image, 5)\n",
    "    else:\n",
    "        raise ValueError('Unrecognized filter')\n",
    "\n",
    "    if display:\n",
    "        return cv2.cvtColor(filtered_image, cv2.COLOR_BGR2RGB)\n",
    "    return filtered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:50:43.825983600Z",
     "start_time": "2023-07-19T06:50:43.800986100Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    image = enhance(image) # returned BGR\n",
    "    image = remove_noise(image, filter='median', display=True) # returned RGB\n",
    "    image = rgb2lab(image) # returned LAB\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[22.50646655, 24.72591622, 25.65447437, ..., 37.24358223,\n        35.14120306, 35.14120306],\n       [24.72591622, 25.61939422, 29.02538145, ..., 37.24358223,\n        35.14120306, 35.14120306],\n       [25.94057896, 29.02538145, 30.33127958, ..., 37.32161247,\n        35.44240552, 35.14120306],\n       ...,\n       [77.59049645, 74.0168619 , 67.39515298, ..., 18.95741947,\n        18.91942872, 15.2620094 ],\n       [74.0168619 , 68.50691067, 61.78662237, ..., 25.34849525,\n        25.34849525, 15.2620094 ],\n       [74.0168619 , 67.39515298, 60.07142015, ..., 25.75760924,\n        25.75760924, 15.2620094 ]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = preprocess('data/selected_dataset/infected/DSC03980_4.JPG')\n",
    "_[:,:,0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:50:45.808241800Z",
     "start_time": "2023-07-19T06:50:45.563063600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:55:07.879963Z",
     "start_time": "2023-07-19T06:55:07.240031600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4340 images belonging to 2 classes.\n",
      "Found 542 images belonging to 2 classes.\n",
      "Found 4340 images belonging to 2 classes.\n",
      "Found 542 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    zoom_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=preprocess\n",
    ")\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess)\n",
    "\n",
    "train_generator_150 = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=(150,150),\n",
    "    class_mode='binary'\n",
    ")\n",
    "val_generator_150 = val_datagen.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    target_size=(150,150),\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "train_generator_224 = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=(224,224),\n",
    "    class_mode='binary'\n",
    ")\n",
    "val_generator_224 = train_datagen.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    target_size=(224,224),\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:55:25.332299900Z",
     "start_time": "2023-07-19T06:55:25.319300100Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (150, 150, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:55:27.131299300Z",
     "start_time": "2023-07-19T06:55:27.110064Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=4,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T03:15:57.607066200Z",
     "start_time": "2023-07-19T03:15:56.501067800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 17, 17, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 15, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 7, 7, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              12846080  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13497153 (51.49 MB)\n",
      "Trainable params: 13497153 (51.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TomConv: An Improved CNN Model for Diagnosis of Diseases in Tomato Plant Leaves\n",
    "# Preeti BaserJatinderkumar R. SainiKetan Kotecha\n",
    "# https://www.sciencedirect.com/science/article/pii/S1877050923001606\n",
    "model1 = Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=INPUT_SHAPE),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(256, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:55:31.497186800Z",
     "start_time": "2023-07-19T06:55:30.307300400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_158 (Conv2D)         (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " batch_normalization_158 (B  (None, 150, 150, 64)      256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPooli  (None, 75, 75, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_159 (Conv2D)         (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_159 (B  (None, 75, 75, 128)       512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPooli  (None, 37, 37, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " conv2d_160 (Conv2D)         (None, 37, 37, 160)       184480    \n",
      "                                                                 \n",
      " batch_normalization_160 (B  (None, 37, 37, 160)       640       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPooli  (None, 18, 18, 160)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 18, 18, 160)       0         \n",
      "                                                                 \n",
      " conv2d_161 (Conv2D)         (None, 18, 18, 224)       322784    \n",
      "                                                                 \n",
      " batch_normalization_161 (B  (None, 18, 18, 224)       896       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPooli  (None, 9, 9, 224)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 9, 9, 224)         0         \n",
      "                                                                 \n",
      " conv2d_162 (Conv2D)         (None, 9, 9, 256)         516352    \n",
      "                                                                 \n",
      " batch_normalization_162 (B  (None, 9, 9, 256)         1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_9 (Avera  (None, 4, 4, 256)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3348097 (12.77 MB)\n",
      "Trainable params: 3346433 (12.77 MB)\n",
      "Non-trainable params: 1664 (6.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# SWP-LeafNET: A novel multistage approach for plant leaf identification based on deep CNN\n",
    "# Ali BeikmohammadiKarim FaezAli Motallebi\n",
    "# https://www.sciencedirect.com/science/article/pii/S0957417422008016\n",
    "# second model\n",
    "model2 = Sequential([\n",
    "    layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(150,150,3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Conv2D(160, (3,3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv2D(224, (3,3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.AveragePooling2D((2,2)),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer='l2'),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "steps_per_epoch = train_generator_150.samples // train_generator_150.batch_size\n",
    "clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=0.001,\n",
    "    maximal_learning_rate=0.006,\n",
    "    scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "    step_size=2 * steps_per_epoch\n",
    ")\n",
    "\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(clr),\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T01:47:55.104455800Z",
     "start_time": "2023-07-18T01:47:53.569836200Z"
    }
   },
   "outputs": [],
   "source": [
    "# SWP-LeafNET: A novel multistage approach for plant leaf identification based on deep CNN\n",
    "# Ali BeikmohammadiKarim FaezAli Motallebi\n",
    "# https://www.sciencedirect.com/science/article/pii/S0957417422008016\n",
    "# third model\n",
    "mobilenet = MobileNetV2(input_shape=(224,224,3),\n",
    "                        include_top=False,\n",
    "                        weights='imagenet')\n",
    "\n",
    "for layer in mobilenet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = layers.Flatten()(mobilenet.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model3 = tf.keras.models.Model(mobilenet.input, x)\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T01:48:09.428796800Z",
     "start_time": "2023-07-18T01:48:04.274365900Z"
    }
   },
   "outputs": [],
   "source": [
    "inceptionresnet = InceptionResNetV2(input_shape=INPUT_SHAPE, weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in inceptionresnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = layers.Flatten()(inceptionresnet.output)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model4 = tf.keras.models.Model(inceptionresnet.input, x)\n",
    "model4.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " lambda_14 (Lambda)          (None, 224, 224)             0         ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_15 (Lambda)          (None, 224, 224, 2)          0         ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_14 (Reshape)        (None, 224, 224, 1)          0         ['lambda_14[0][0]']           \n",
      "                                                                                                  \n",
      " reshape_15 (Reshape)        (None, 224, 224, 2)          0         ['lambda_15[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_150 (Conv2D)         (None, 111, 111, 3)          30        ['reshape_14[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_153 (Conv2D)         (None, 111, 111, 19)         361       ['reshape_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_150 (B  (None, 111, 111, 3)          12        ['conv2d_150[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_153 (B  (None, 111, 111, 19)         76        ['conv2d_153[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_151 (Conv2D)         (None, 109, 109, 3)          84        ['batch_normalization_150[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_154 (Conv2D)         (None, 109, 109, 19)         3268      ['batch_normalization_153[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_151 (B  (None, 109, 109, 3)          12        ['conv2d_151[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_154 (B  (None, 109, 109, 19)         76        ['conv2d_154[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_152 (Conv2D)         (None, 109, 109, 13)         364       ['batch_normalization_151[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_155 (Conv2D)         (None, 109, 109, 13)         2236      ['batch_normalization_154[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_152 (B  (None, 109, 109, 13)         52        ['conv2d_152[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_155 (B  (None, 109, 109, 13)         52        ['conv2d_155[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_25 (MaxPooli  (None, 54, 54, 13)           0         ['batch_normalization_152[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " max_pooling2d_26 (MaxPooli  (None, 54, 54, 13)           0         ['batch_normalization_155[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 54, 54, 26)           0         ['max_pooling2d_25[0][0]',    \n",
      " )                                                                   'max_pooling2d_26[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_156 (Conv2D)         (None, 54, 54, 80)           2160      ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_156 (B  (None, 54, 54, 80)           320       ['conv2d_156[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_157 (Conv2D)         (None, 52, 52, 192)          138432    ['batch_normalization_156[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_157 (B  (None, 52, 52, 192)          768       ['conv2d_157[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_27 (MaxPooli  (None, 26, 26, 192)          0         ['batch_normalization_157[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2  (None, 192)                  0         ['max_pooling2d_27[0][0]']    \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    193       ['global_average_pooling2d_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 148496 (580.06 KB)\n",
      "Trainable params: 147812 (577.39 KB)\n",
      "Non-trainable params: 684 (2.67 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l_filters = 6\n",
    "ab_filters = 26\n",
    "l_filters_3rd = 13\n",
    "ab_filters_3rd = 51\n",
    "\n",
    "input = layers.Input(shape=(224,224,3))\n",
    "l_cnl = layers.Lambda(lambda x: x[:,:,:,0])(input)\n",
    "l_cnl = layers.Reshape((224,224,1))(l_cnl)\n",
    "l_cnl = layers.Conv2D(l_filters/2, (3,3), strides=2, activation='relu')(l_cnl)\n",
    "l_cnl = layers.BatchNormalization()(l_cnl)\n",
    "l_cnl = layers.Conv2D(l_filters/2, (3,3), activation='relu')(l_cnl)\n",
    "l_cnl = layers.BatchNormalization()(l_cnl)\n",
    "l_cnl = layers.Conv2D(l_filters_3rd, (3,3), padding='same', activation='relu')(l_cnl)\n",
    "l_cnl = layers.BatchNormalization()(l_cnl)\n",
    "l_cnl = layers.MaxPooling2D(2)(l_cnl)\n",
    "\n",
    "# # ab_input = layers.Input(shape=(None,224,224,3))\n",
    "ab_cnl = layers.Lambda(lambda x: x[:,:,:,1:])(input)\n",
    "ab_cnl = layers.Reshape((224,224,2))(ab_cnl)\n",
    "ab_cnl = layers.Conv2D(32-ab_filters/2, (3,3), strides=2, activation='relu')(ab_cnl)\n",
    "ab_cnl = layers.BatchNormalization()(ab_cnl)\n",
    "ab_cnl = layers.Conv2D(32-ab_filters/2, (3,3), activation='relu')(ab_cnl)\n",
    "ab_cnl = layers.BatchNormalization()(ab_cnl)\n",
    "ab_cnl = layers.Conv2D(64-ab_filters_3rd, (3,3), padding='same', activation='relu')(ab_cnl)\n",
    "ab_cnl = layers.BatchNormalization()(ab_cnl)\n",
    "ab_cnl = layers.MaxPooling2D(2)(ab_cnl)\n",
    "\n",
    "x = layers.Concatenate()([l_cnl, ab_cnl])\n",
    "x = layers.Conv2D(80, (1,1), activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(192, (3,3), activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# inceptionv3 = InceptionV3(weights='imagenet', include_top=False)\n",
    "# for inc_layers in inceptionv3.layers:\n",
    "#     inc_layers.trainable = False\n",
    "# inception_x6 = inceptionv3.get_layer('mixed6')\n",
    "# x = inception_x6([x])\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model5 = tf.keras.models.Model(input, x)\n",
    "model5.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T06:06:35.363432700Z",
     "start_time": "2023-07-19T06:06:35.024240700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:12:10.142280Z",
     "start_time": "2023-07-18T01:48:09.433797100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/105\n",
      "109/109 [==============================] - 134s 1s/step - loss: 0.8932 - accuracy: 0.5147 - val_loss: 0.7443 - val_accuracy: 0.4977\n",
      "Epoch 2/105\n",
      "109/109 [==============================] - 100s 918ms/step - loss: 0.7098 - accuracy: 0.5043 - val_loss: 0.7051 - val_accuracy: 0.5000\n",
      "Epoch 3/105\n",
      "109/109 [==============================] - 94s 859ms/step - loss: 0.6952 - accuracy: 0.5228 - val_loss: 0.6902 - val_accuracy: 0.5565\n",
      "Epoch 4/105\n",
      "109/109 [==============================] - 94s 859ms/step - loss: 0.6942 - accuracy: 0.5170 - val_loss: 0.7325 - val_accuracy: 0.5000\n",
      "Epoch 5/105\n",
      "109/109 [==============================] - 95s 869ms/step - loss: 0.6961 - accuracy: 0.5049 - val_loss: 0.6971 - val_accuracy: 0.4977\n",
      "Epoch 6/105\n",
      "109/109 [==============================] - 105s 962ms/step - loss: 0.6943 - accuracy: 0.4925 - val_loss: 0.6891 - val_accuracy: 0.5576\n",
      "Epoch 7/105\n",
      "109/109 [==============================] - 105s 961ms/step - loss: 0.6935 - accuracy: 0.5115 - val_loss: 0.6903 - val_accuracy: 0.5242\n",
      "Epoch 8/105\n",
      "109/109 [==============================] - 102s 930ms/step - loss: 0.6928 - accuracy: 0.5288 - val_loss: 0.6892 - val_accuracy: 0.5196\n",
      "Epoch 9/105\n",
      "109/109 [==============================] - 102s 939ms/step - loss: 0.6962 - accuracy: 0.5032 - val_loss: 0.6917 - val_accuracy: 0.5161\n",
      "Epoch 10/105\n",
      "109/109 [==============================] - 107s 977ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.6833 - val_accuracy: 0.5853\n",
      "Epoch 11/105\n",
      "109/109 [==============================] - 103s 945ms/step - loss: 0.6927 - accuracy: 0.5112 - val_loss: 0.6938 - val_accuracy: 0.5069\n",
      "Epoch 12/105\n",
      "109/109 [==============================] - 99s 906ms/step - loss: 0.6933 - accuracy: 0.5204 - val_loss: 0.6917 - val_accuracy: 0.5219\n",
      "Epoch 13/105\n",
      "109/109 [==============================] - 100s 917ms/step - loss: 0.6920 - accuracy: 0.5271 - val_loss: 0.6906 - val_accuracy: 0.5081\n",
      "Epoch 14/105\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5135Restoring model weights from the end of the best epoch: 10.\n",
      "109/109 [==============================] - 99s 906ms/step - loss: 0.6931 - accuracy: 0.5135 - val_loss: 0.6942 - val_accuracy: 0.4977\n",
      "Epoch 14: early stopping\n",
      "INFO:tensorflow:Assets written to: dumps\\model1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dumps\\model1\\assets\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_generator_150, validation_data=val_generator_150,\n",
    "           epochs=105, callbacks=[early_stopping]) # epochs 19 val_accuracy=0.54\n",
    "model1.save(os.path.join('dumps', 'model1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T08:06:51.218027200Z",
     "start_time": "2023-07-19T06:55:52.500100700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/105\n",
      "136/136 [==============================] - 392s 3s/step - loss: 4.9338 - accuracy: 0.5661 - val_loss: 0.9453 - val_accuracy: 0.5387\n",
      "Epoch 2/105\n",
      "136/136 [==============================] - 333s 2s/step - loss: 0.7982 - accuracy: 0.5597 - val_loss: 0.7291 - val_accuracy: 0.5018\n",
      "Epoch 3/105\n",
      "136/136 [==============================] - 302s 2s/step - loss: 0.7408 - accuracy: 0.5359 - val_loss: 0.7040 - val_accuracy: 0.5055\n",
      "Epoch 4/105\n",
      "136/136 [==============================] - 293s 2s/step - loss: 0.6913 - accuracy: 0.5988 - val_loss: 0.6757 - val_accuracy: 0.6199\n",
      "Epoch 5/105\n",
      "136/136 [==============================] - 291s 2s/step - loss: 0.6880 - accuracy: 0.6071 - val_loss: 0.7085 - val_accuracy: 0.5572\n",
      "Epoch 6/105\n",
      "136/136 [==============================] - 289s 2s/step - loss: 0.7042 - accuracy: 0.5968 - val_loss: 0.7095 - val_accuracy: 0.6107\n",
      "Epoch 7/105\n",
      "136/136 [==============================] - 293s 2s/step - loss: 0.6930 - accuracy: 0.6065 - val_loss: 0.6798 - val_accuracy: 0.6181\n",
      "Epoch 8/105\n",
      "136/136 [==============================] - 294s 2s/step - loss: 0.6771 - accuracy: 0.6092 - val_loss: 0.6732 - val_accuracy: 0.6218\n",
      "Epoch 9/105\n",
      "136/136 [==============================] - 304s 2s/step - loss: 0.6643 - accuracy: 0.6171 - val_loss: 0.6534 - val_accuracy: 0.6310\n",
      "Epoch 10/105\n",
      "136/136 [==============================] - 304s 2s/step - loss: 0.6729 - accuracy: 0.6267 - val_loss: 0.6642 - val_accuracy: 0.6494\n",
      "Epoch 11/105\n",
      "136/136 [==============================] - 306s 2s/step - loss: 0.6676 - accuracy: 0.6230 - val_loss: 0.6688 - val_accuracy: 0.6218\n",
      "Epoch 12/105\n",
      "136/136 [==============================] - 278s 2s/step - loss: 0.6607 - accuracy: 0.6253 - val_loss: 0.6483 - val_accuracy: 0.6347\n",
      "Epoch 13/105\n",
      "136/136 [==============================] - 293s 2s/step - loss: 0.6537 - accuracy: 0.6325 - val_loss: 0.8086 - val_accuracy: 0.5461\n",
      "Epoch 14/105\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.6251Restoring model weights from the end of the best epoch: 10.\n",
      "136/136 [==============================] - 280s 2s/step - loss: 0.6583 - accuracy: 0.6251 - val_loss: 0.6524 - val_accuracy: 0.6292\n",
      "Epoch 14: early stopping\n",
      "INFO:tensorflow:Assets written to: dumps\\model2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dumps\\model2\\assets\n"
     ]
    }
   ],
   "source": [
    "model2.fit(train_generator_150, validation_data=val_generator_150,\n",
    "           epochs=105, callbacks=[early_stopping])\n",
    "model2.save(os.path.join('dumps', 'model2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T03:41:23.988768400Z",
     "start_time": "2023-07-18T02:50:06.856640700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/105\n",
      "109/109 [==============================] - 205s 2s/step - loss: 2.9158 - accuracy: 0.5109 - val_loss: 0.6597 - val_accuracy: 0.6094\n",
      "Epoch 2/105\n",
      "109/109 [==============================] - 198s 2s/step - loss: 0.7195 - accuracy: 0.5495 - val_loss: 0.7090 - val_accuracy: 0.5392\n",
      "Epoch 3/105\n",
      "109/109 [==============================] - 195s 2s/step - loss: 0.7316 - accuracy: 0.5452 - val_loss: 0.6681 - val_accuracy: 0.5956\n",
      "Epoch 4/105\n",
      "109/109 [==============================] - 201s 2s/step - loss: 0.7177 - accuracy: 0.5421 - val_loss: 0.7115 - val_accuracy: 0.5507\n",
      "Epoch 5/105\n",
      "109/109 [==============================] - 205s 2s/step - loss: 0.6806 - accuracy: 0.5907 - val_loss: 0.6843 - val_accuracy: 0.5899\n",
      "Epoch 6/105\n",
      "109/109 [==============================] - 207s 2s/step - loss: 0.6786 - accuracy: 0.5772 - val_loss: 0.7021 - val_accuracy: 0.5173\n",
      "Epoch 7/105\n",
      "109/109 [==============================] - 202s 2s/step - loss: 0.6779 - accuracy: 0.5858 - val_loss: 0.6749 - val_accuracy: 0.5726\n",
      "Epoch 8/105\n",
      "109/109 [==============================] - 221s 2s/step - loss: 0.6780 - accuracy: 0.5697 - val_loss: 0.6652 - val_accuracy: 0.6129\n",
      "Epoch 9/105\n",
      "109/109 [==============================] - 206s 2s/step - loss: 0.6744 - accuracy: 0.5766 - val_loss: 0.6559 - val_accuracy: 0.6290\n",
      "Epoch 10/105\n",
      "109/109 [==============================] - 203s 2s/step - loss: 0.6675 - accuracy: 0.5962 - val_loss: 0.7139 - val_accuracy: 0.5369\n",
      "Epoch 11/105\n",
      "109/109 [==============================] - 201s 2s/step - loss: 0.6757 - accuracy: 0.6017 - val_loss: 0.6314 - val_accuracy: 0.6475\n",
      "Epoch 12/105\n",
      "109/109 [==============================] - 202s 2s/step - loss: 0.6663 - accuracy: 0.5942 - val_loss: 0.6558 - val_accuracy: 0.6141\n",
      "Epoch 13/105\n",
      "109/109 [==============================] - 204s 2s/step - loss: 0.6688 - accuracy: 0.6063 - val_loss: 0.6699 - val_accuracy: 0.6094\n",
      "Epoch 14/105\n",
      "109/109 [==============================] - 213s 2s/step - loss: 0.6691 - accuracy: 0.6022 - val_loss: 0.6597 - val_accuracy: 0.6429\n",
      "Epoch 15/105\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.6660 - accuracy: 0.5927Restoring model weights from the end of the best epoch: 11.\n",
      "109/109 [==============================] - 203s 2s/step - loss: 0.6660 - accuracy: 0.5927 - val_loss: 0.6503 - val_accuracy: 0.6118\n",
      "Epoch 15: early stopping\n",
      "INFO:tensorflow:Assets written to: dumps\\model3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dumps\\model3\\assets\n"
     ]
    }
   ],
   "source": [
    "model3.fit(train_generator_224, validation_data=val_generator_224,\n",
    "           epochs=105, callbacks=[early_stopping])\n",
    "model3.save(os.path.join('dumps', 'model3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T04:06:25.368765700Z",
     "start_time": "2023-07-18T03:41:24.000767800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/105\n",
      "109/109 [==============================] - 138s 1s/step - loss: 31.9992 - accuracy: 0.4798 - val_loss: 24.5827 - val_accuracy: 0.5046\n",
      "Epoch 2/105\n",
      "109/109 [==============================] - 128s 1s/step - loss: 17.6680 - accuracy: 0.4891 - val_loss: 36.4228 - val_accuracy: 0.5012\n",
      "Epoch 3/105\n",
      "109/109 [==============================] - 129s 1s/step - loss: 13.3266 - accuracy: 0.5092 - val_loss: 5.1855 - val_accuracy: 0.5127\n",
      "Epoch 4/105\n",
      "109/109 [==============================] - 137s 1s/step - loss: 5.7065 - accuracy: 0.4948 - val_loss: 1.9225 - val_accuracy: 0.5081\n",
      "Epoch 5/105\n",
      "109/109 [==============================] - 133s 1s/step - loss: 3.3969 - accuracy: 0.5127 - val_loss: 6.6688 - val_accuracy: 0.5046\n",
      "Epoch 6/105\n",
      "109/109 [==============================] - 133s 1s/step - loss: 2.7693 - accuracy: 0.5207 - val_loss: 1.4295 - val_accuracy: 0.4839\n",
      "Epoch 7/105\n",
      "109/109 [==============================] - 132s 1s/step - loss: 2.1191 - accuracy: 0.5037 - val_loss: 0.8661 - val_accuracy: 0.5219\n",
      "Epoch 8/105\n",
      "109/109 [==============================] - 131s 1s/step - loss: 2.7048 - accuracy: 0.5049 - val_loss: 1.2823 - val_accuracy: 0.5115\n",
      "Epoch 9/105\n",
      "109/109 [==============================] - 132s 1s/step - loss: 2.6521 - accuracy: 0.4977 - val_loss: 2.0103 - val_accuracy: 0.5104\n",
      "Epoch 10/105\n",
      "109/109 [==============================] - 132s 1s/step - loss: 2.3813 - accuracy: 0.5032 - val_loss: 0.9871 - val_accuracy: 0.4908\n",
      "Epoch 11/105\n",
      "109/109 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.5135Restoring model weights from the end of the best epoch: 7.\n",
      "109/109 [==============================] - 133s 1s/step - loss: 1.3394 - accuracy: 0.5135 - val_loss: 1.0491 - val_accuracy: 0.5207\n",
      "Epoch 11: early stopping\n",
      "INFO:tensorflow:Assets written to: dumps\\model4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dumps\\model4\\assets\n"
     ]
    }
   ],
   "source": [
    "model4.fit(train_generator_150, validation_data=val_generator_150,\n",
    "           epochs=105, callbacks=[early_stopping])\n",
    "model4.save(os.path.join('dumps', 'model4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
